{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch import optim\n",
    "import torch.nn.functional as f\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import unicodedata\n",
    "import codecs\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_path = os.path.join('cornell movie-dialogs corpus','movie_lines.txt')\n",
    "conv_path = os.path.join('cornell movie-dialogs corpus','movie_conversations.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!'\n",
      "b'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!'\n",
      "b'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.'\n",
      "b'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?'\n",
      "b\"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\"\n",
      "b'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow'\n",
      "b\"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\"\n",
      "b'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No'\n",
      "b'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?'\n",
      "b'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?'\n"
     ]
    }
   ],
   "source": [
    "with open(line_path, 'rb') as f:\n",
    "    lines = f.readlines()\n",
    "#   lines = lines.encode('utf-8').strip()\n",
    "    \n",
    "for line in lines[:10]:\n",
    "    print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_fields = ['lineID', 'charID', 'movieID', 'char', 'text']\n",
    "lines = {}\n",
    "with open(line_path, 'r', encoding='iso-8859-1') as f:\n",
    "    for line in f:\n",
    "        value = line.split(' +++$+++ ')\n",
    "        lineobj = {}\n",
    "        for i , field in enumerate(line_fields):\n",
    "            lineobj[field] = value[i]\n",
    "        lines[lineobj['lineID']] = lineobj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lineID': 'L1045',\n",
       " 'charID': 'u0',\n",
       " 'movieID': 'm0',\n",
       " 'char': 'BIANCA',\n",
       " 'text': 'They do not!\\n'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(lines.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\"\n",
      "b\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\"\n",
      "b\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\"\n",
      "b\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\"\n",
      "b\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\"\n",
      "b\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L271', 'L272', 'L273', 'L274', 'L275']\"\n",
      "b\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L276', 'L277']\"\n",
      "b\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L280', 'L281']\"\n",
      "b\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L363', 'L364']\"\n",
      "b\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L365', 'L366']\"\n"
     ]
    }
   ],
   "source": [
    "with open(conv_path, 'rb') as f:\n",
    "    ls = f.readlines()\n",
    "#   lines = lines.encode('utf-8').strip()\n",
    "    \n",
    "for line in ls[:10]:\n",
    "    print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_fields = ['char1ID', 'char2ID', 'movieID', 'utteranceID']\n",
    "convs = []\n",
    "with open(conv_path, 'r', encoding='iso-8859-1') as f:\n",
    "    for line in f:\n",
    "        value = line.split(' +++$+++ ')\n",
    "        convobj = {}\n",
    "        for i , field in enumerate(conv_fields):\n",
    "            convobj[field] = value[i]\n",
    "        lineIDs = eval(convobj['utteranceID'])\n",
    "        convobj['lines'] = []\n",
    "        for lineID in lineIDs:\n",
    "            convobj['lines'].append(lines[lineID])\n",
    "        convs.append(convobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83097"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(convs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "for conversation in convs:\n",
    "    for i in range(len(conversation['lines']) - 1):\n",
    "        questline = conversation['lines'][i]['text'].strip()\n",
    "        ansline = conversation['lines'][i+1]['text'].strip()\n",
    "        if questline and ansline:\n",
    "            qa_pairs.append([questline, ansline])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.', \"Well, I thought we'd start with pronunciation, if that's okay with you.\"]\n"
     ]
    }
   ],
   "source": [
    "print(qa_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221282"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = os.path.join('cornell movie-dialogs corpus', 'formatted_movie_lines.txt')\n",
    "delimiter = '\\t'\n",
    "\n",
    "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
    "\n",
    "with open(datafile, 'w', encoding='utf-8') as of:\n",
    "    writer = csv.writer(of, delimiter=delimiter)\n",
    "    for pair in qa_pairs:\n",
    "        writer.writerow(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\r\\n\"\n",
      "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\r\\n\"\n",
      "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\r\\n\"\n",
      "b\"You're asking me out.  That's so cute. What's your name again?\\tForget it.\\r\\n\"\n",
      "b\"No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\\r\\n\"\n"
     ]
    }
   ],
   "source": [
    "with open(datafile, 'rb') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "for line in lines[:5]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = 0\n",
    "s_token = 1\n",
    "e_token = 2\n",
    "\n",
    "class vocabulary:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2idx = {}\n",
    "        self.word2count = {}\n",
    "        self.idx2word = {pad_token: 'pad', s_token: 'sos', e_token: 'eos'}\n",
    "        self.num_words = 3\n",
    "        \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.idx2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split():\n",
    "            self.addWord(word)\n",
    "            \n",
    "    def trim(self, min_count):\n",
    "        for k, v in self.word2count.items():\n",
    "            if v <= min_count:\n",
    "                del self.idx2word[self.word2idx[k]]\n",
    "                del self.word2idx[k]\n",
    "#               del self.word2count[k]   \n",
    "                \n",
    "        print('Number of words in vocab: {}'.format(len(self.word2idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'helloworld ? ! !'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalise(\"helloWorld 1234?!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = os.path.join('cornell movie-dialogs corpus', 'formatted_movie_lines.txt')\n",
    "\n",
    "lines = open(datafile, encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "pairs = [[normalise(s) for s in pair.split('\\t')] for pair in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['can we make this quick ? roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad . again .',\n",
       "  'well i thought we d start with pronunciation if that s okay with you .'],\n",
       " ['well i thought we d start with pronunciation if that s okay with you .',\n",
       "  'not the hacking and gagging and spitting part . please .'],\n",
       " ['not the hacking and gagging and spitting part . please .',\n",
       "  'okay . . . then how bout we try out some french cuisine . saturday ? night ?'],\n",
       " ['you re asking me out . that s so cute . what s your name again ?',\n",
       "  'forget it .'],\n",
       " ['no no it s my fault we didn t have a proper introduction', 'cameron .']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221282"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterSents(s, max_len=13, min_len=1):\n",
    "    newPairs = []\n",
    "    for pair in pairs:\n",
    "        boo = len(pair[0].split())< max_len and len(pair[1].split())< max_len\n",
    "        boo = (len(pair[0].split())> min_len and len(pair[1].split())> min_len) and boo\n",
    "        if(boo):\n",
    "            newPairs.append(pair)\n",
    "            \n",
    "    return newPairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPairs = filterSents(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93385"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newPairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vocabulary('cornell movie-dialogs corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words 23621\n"
     ]
    }
   ],
   "source": [
    "for pair in newPairs:\n",
    "    vocab.addSentence(pair[0])\n",
    "    vocab.addSentence(pair[1])\n",
    "\n",
    "print('Number of words', vocab.num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeRare(vocab, pairs, thresh=3):\n",
    "    vocab.trim(thresh)\n",
    "    final_pairs = []\n",
    "    \n",
    "    for i, pair in enumerate(pairs):\n",
    "        flag = 1\n",
    "        for word in pair[0].split():\n",
    "            if word not in vocab.word2idx:\n",
    "#                 pairs.remove(pairs[i])\n",
    "                flag = 0\n",
    "        for word in pair[1].split():\n",
    "            if word not in vocab.word2idx:\n",
    "#                 pairs.remove(pairs[i])\n",
    "                flag = 0\n",
    "        if flag==1:\n",
    "            final_pairs.append(pair)\n",
    "                \n",
    "    return final_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x', 'y']\n"
     ]
    }
   ],
   "source": [
    "l = ['l','x','y']\n",
    "l.remove('l')\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 8997\n"
     ]
    }
   ],
   "source": [
    "final_pairs = removeRare(vocab, newPairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['gosh if only we could find kat a boyfriend . . .', 'let me see what i can do .'], ['that s because it s such a nice one .', 'forget french .'], ['there .', 'where ?'], ['you have my word . as a gentleman', 'you re sweet .'], ['hi .', 'looks like things worked out tonight huh ?']]\n"
     ]
    }
   ],
   "source": [
    "print(final_pairs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75020"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_idxs(vocab, sent):\n",
    "    return [vocab.word2idx[word] for word in sent.split()] + [e_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forget french . [43, 44, 12, 2]\n"
     ]
    }
   ],
   "source": [
    "print(final_pairs[1][1], sent_idxs(vocab, final_pairs[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = []\n",
    "out = []\n",
    "\n",
    "for pair in final_pairs[:10]:\n",
    "    inp.append(pair[0])\n",
    "    out.append(pair[1])\n",
    "    \n",
    "indxs = [sent_idxs(vocab, sentence) for sentence in inp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gosh if only we could find kat a boyfriend . . .', 'that s because it s such a nice one .', 'there .', 'you have my word . as a gentleman', 'hi .'] [[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 12, 12, 2], [36, 37, 38, 39, 37, 40, 10, 41, 42, 12, 2], [53, 12, 2], [30, 59, 26, 60, 12, 61, 10, 62, 2], [64, 12, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(inp[:5],indxs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroPadding(l, val=0):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = zeroPadding(indxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 36, 53, 30, 64, 17, 52, 19, 17, 16),\n",
       " (4, 37, 12, 59, 12, 79, 86, 30, 93, 96),\n",
       " (5, 38, 2, 26, 2, 29, 12, 91, 30, 97),\n",
       " (6, 39, 0, 60, 0, 2, 12, 82, 94, 29),\n",
       " (7, 37, 0, 12, 0, 0, 12, 24, 95, 2),\n",
       " (8, 40, 0, 61, 0, 0, 2, 92, 82, 0),\n",
       " (9, 10, 0, 10, 0, 0, 0, 29, 34, 0),\n",
       " (10, 41, 0, 62, 0, 0, 0, 2, 96, 0),\n",
       " (11, 42, 0, 2, 0, 0, 0, 0, 97, 0),\n",
       " (12, 12, 0, 0, 0, 0, 0, 0, 98, 0),\n",
       " (12, 2, 0, 0, 0, 0, 0, 0, 12, 0),\n",
       " (12, 0, 0, 0, 0, 0, 0, 0, 2, 0),\n",
       " (2, 0, 0, 0, 0, 0, 0, 0, 0, 0)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binaryMatrix(l):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token==pad_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "                \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_b = binaryMatrix(test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 0, 1, 0, 1, 1, 1, 1, 1], [1, 1, 0, 1, 0, 0, 1, 1, 1, 1], [1, 1, 0, 1, 0, 0, 1, 1, 1, 0], [1, 1, 0, 1, 0, 0, 0, 1, 1, 0], [1, 1, 0, 1, 0, 0, 0, 1, 1, 0], [1, 1, 0, 1, 0, 0, 0, 0, 1, 0], [1, 1, 0, 0, 0, 0, 0, 0, 1, 0], [1, 1, 0, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(test_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputs(l, voc):\n",
    "    indxs_batch = [sent_idxs(vocab, sent) for sent in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indxs_batch])\n",
    "    padded = zeroPadding(indxs_batch)\n",
    "    pad_tens = torch.LongTensor(padded)\n",
    "    \n",
    "    return pad_tens, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputs(l, voc):\n",
    "    indxs_batch = [sent_idxs(vocab, sent) for sent in l]\n",
    "    max_len = max([len(indexes) for indexes in indxs_batch])\n",
    "    padded = zeroPadding(indxs_batch)\n",
    "    pad_tens = torch.LongTensor(padded)\n",
    "    mask = binaryMatrix(padded)\n",
    "    mask = torch.ByteTensor(mask)\n",
    "    \n",
    "    return pad_tens, mask, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch2data(vocab, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split()), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputs(input_batch, vocab)\n",
    "    out, mask, max_target_len = outputs(output_batch, vocab)\n",
    "    \n",
    "    return inp, lengths, out, mask, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  tensor([[   17,   571,    45,    16, 10439],\n",
      "        [  166,    30,    37,    29,  3323],\n",
      "        [  212,   881,   112,   109,    12],\n",
      "        [   82,    12,  2765,     2,     2],\n",
      "        [  854,    96,  7935,     0,     0],\n",
      "        [   30,   191,    29,     0,     0],\n",
      "        [  199,    12,     2,     0,     0],\n",
      "        [  108,     2,     0,     0,     0],\n",
      "        [  360,     0,     0,     0,     0],\n",
      "        [  126,     0,     0,     0,     0],\n",
      "        [   12,     0,     0,     0,     0],\n",
      "        [    2,     0,     0,     0,     0]])\n",
      "Lengths:  tensor([12,  8,  7,  4,  4])\n",
      "Output:  tensor([[ 146,   18,   80,  116, 3323],\n",
      "        [  82,   17, 1779,   37,   12],\n",
      "        [  95,   15,  344,  818,   16],\n",
      "        [  96,   30,   12, 2290,   37],\n",
      "        [ 510,  288,  139,  509,  116],\n",
      "        [ 711,   29,   34,   12,  319],\n",
      "        [ 706,    2, 7999,   13,   82],\n",
      "        [  12,    0,   12,   37,   90],\n",
      "        [   2,    0,   66,   19,   33],\n",
      "        [   0,    0,  179,  267,  904],\n",
      "        [   0,    0,  454,  896,   29],\n",
      "        [   0,    0,   12,   29,    2],\n",
      "        [   0,    0,    2,    2,    0]])\n",
      "Mask:  tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 0, 1, 1, 1],\n",
      "        [1, 0, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 0]], dtype=torch.uint8)\n",
      "Max Len:  13\n"
     ]
    }
   ],
   "source": [
    "small_batch = 5\n",
    "batches = batch2data(vocab, [random.choice(final_pairs) for _ in range(small_batch)])\n",
    "inp, lengths, out, mask, max_target_len = batches\n",
    "\n",
    "print('Input: ', inp)\n",
    "print('Lengths: ', lengths)\n",
    "print('Output: ', out)\n",
    "print('Mask: ', mask)\n",
    "print('Max Len: ', max_target_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module): # nn.Module contain PyTorch's neural network objects\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size # hidden_size is the number of neurons in the hidden layer\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout), bidirectional = True)\n",
    "        \n",
    "        # Input Size and Hidden Size are both set as 'hidden_size' because our input size is a word embedding \n",
    "        # with number of features == hidden_size\n",
    "        \n",
    "    def forward(self, input_seq, input_lengths, hidden=None): # Overwriting the default forward prop function\n",
    "        # input_seq: (max_len, batch_size)\n",
    "        # input_length: the list containing the length of sentences in the batch\n",
    "        # output_seq: (seq_len, batch_size, hidden_size*n_directions)\n",
    "        # hidden_state: (n_layers*n_directions, batch_size, hidden_size)\n",
    "        \n",
    "        embedded = self.embedding(input_seq)\n",
    "        \n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        \n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        \n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # Sum of bidirectional GRU Output:\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
    "        \n",
    "        return outputs, hidden\n",
    "    \n",
    "    # Remember that the shape of a torch tensor is (no. of channels, rows, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def ele_prod(self, emb_out, dec_hidd):\n",
    "        return torch.sum(emb_out*dec_hidd, dim = 2) # Taking element-wise product and then taking sum across the columns\n",
    "    \n",
    "    def forward(self, emb_out, dec_hidd):\n",
    "        attn_energies = self.ele_prod(emb_out, dec_hidd) # (max_len, batch_size, hidden_size)*(1, batch_size, hidden_size) = (max_len, batch_size, hidden_size)\n",
    "        # attn_energies = (max_len, batch_size)\n",
    "        attn_energies = attn_energies.t() # (batch_size, max_len)\n",
    "        \n",
    "        return f.softmax(attn_energies, dim=1).unsqueeze(1) # (batch_size, 1, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers = 1, dropout = 0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.attn_model = attn_model\n",
    "        self.embedding = embedding\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers==0 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.attn = Attention(attn_model, hidden_size)\n",
    "                          \n",
    "\n",
    "    def forward(self, input_step, last_hidden, enc_out):\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        \n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        \n",
    "        attn_weights = self.attn(rnn_output, enc_out)\n",
    "        context = attn_weights.bmm(enc_out.transpose(0,1))\n",
    "        \n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        \n",
    "        context = context.squeeze(1)\n",
    "        \n",
    "        concat_inp = torch.cat((rnn_output, context), 1)\n",
    "        concat_out = torch.tanh(self.concat(concat_inp))\n",
    "        \n",
    "        output = self.out(concat_out)\n",
    "        output = f.softmax(output, dim=1)\n",
    "        \n",
    "        return output, hidden\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskedNLL(dec_out, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    target = target.view(-1,1)\n",
    "    \n",
    "    gathered_loss = torch.gather(dec_out, 1, target)\n",
    "    crossEnt = -torch.log(gathered_loss)\n",
    "    \n",
    "    loss = crossEnt.masked_select(mask)\n",
    "    \n",
    "    loss = loss.mean().to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  tensor([[ 729,  162, 1158,   17,  185],\n",
      "        [4478,   30, 5647,   19,  107],\n",
      "        [ 109,   34,   39,  225,   12],\n",
      "        [  45, 7249,   37, 1004,    2],\n",
      "        [  34,  101, 1421,  570,    0],\n",
      "        [ 729, 9432, 1421,   12,    0],\n",
      "        [ 162, 2580, 2199,    2,    0],\n",
      "        [ 303,   29,   12,    0,    0],\n",
      "        [  29,    2,    2,    0,    0],\n",
      "        [   2,    0,    0,    0,    0]])\n",
      "Lengths:  tensor([10,  9,  9,  7,  4])\n",
      "Output:  tensor([[  394,     4,   202,    30,  1215],\n",
      "        [  996,    53,  1158,   219,    12],\n",
      "        [   12,   229,    58,    14,   175],\n",
      "        [  394,    10,    37,   106,   208],\n",
      "        [   12,  7249, 13126,  2667,    30],\n",
      "        [   45,    17,    29,    12,    72],\n",
      "        [  162,   307,    45,     2,    39],\n",
      "        [   30,    57,   162,     0,    12],\n",
      "        [   29,    34,    30,     0,     2],\n",
      "        [    2,    42,    29,     0,     0],\n",
      "        [    0,    12,     2,     0,     0],\n",
      "        [    0,     2,     0,     0,     0]])\n",
      "Mask:  tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 0],\n",
      "        [0, 1, 1, 0, 0],\n",
      "        [0, 1, 0, 0, 0]], dtype=torch.uint8)\n",
      "Max Len:  12\n",
      "Initial Decoder Hidden Shape:  torch.Size([2, 5, 500])\n",
      "\n",
      "\n",
      "---------------------------------------- \n",
      "\n",
      "Decoder Output Shape: torch.Size([1, 5])\n",
      "Decoder Hidden Shape: torch.Size([2, 5, 500])\n",
      "Target Variable 1:  tensor([ 394,    4,  202,   30, 1215])\n",
      "Target Variable 1 Shape:  torch.Size([5])\n",
      "Decoder Input Shape:  torch.Size([1, 5])\n",
      "Mask at the present time step:  tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor(10.0880, grad_fn=<AddBackward0>) \n",
      "\n",
      "[50.439839363098145]\n",
      "---------------------------------------- \n",
      "\n",
      "---------------------------------------- \n",
      "\n",
      "Decoder Output Shape: torch.Size([1, 5])\n",
      "Decoder Hidden Shape: torch.Size([2, 5, 500])\n",
      "Target Variable 1:  tensor([ 996,   53, 1158,  219,   12])\n",
      "Target Variable 1 Shape:  torch.Size([5])\n",
      "Decoder Input Shape:  torch.Size([1, 5])\n",
      "Mask at the present time step:  tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor(20.1706, grad_fn=<AddBackward0>) \n",
      "\n",
      "[50.439839363098145, 50.41316032409668]\n",
      "---------------------------------------- \n",
      "\n",
      "---------------------------------------- \n",
      "\n",
      "Decoder Output Shape: torch.Size([1, 5])\n",
      "Decoder Hidden Shape: torch.Size([2, 5, 500])\n",
      "Target Variable 1:  tensor([ 12, 229,  58,  14, 175])\n",
      "Target Variable 1 Shape:  torch.Size([5])\n",
      "Decoder Input Shape:  torch.Size([1, 5])\n",
      "Mask at the present time step:  tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor(30.2834, grad_fn=<AddBackward0>) \n",
      "\n",
      "[50.439839363098145, 50.41316032409668, 50.56408405303955]\n",
      "---------------------------------------- \n",
      "\n",
      "---------------------------------------- \n",
      "\n",
      "Decoder Output Shape: torch.Size([1, 5])\n",
      "Decoder Hidden Shape: torch.Size([2, 5, 500])\n",
      "Target Variable 1:  tensor([394,  10,  37, 106, 208])\n",
      "Target Variable 1 Shape:  torch.Size([5])\n",
      "Decoder Input Shape:  torch.Size([1, 5])\n",
      "Mask at the present time step:  tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor(40.3417, grad_fn=<AddBackward0>) \n",
      "\n",
      "[50.439839363098145, 50.41316032409668, 50.56408405303955, 50.29135227203369]\n",
      "---------------------------------------- \n",
      "\n",
      "---------------------------------------- \n",
      "\n",
      "Decoder Output Shape: torch.Size([1, 5])\n",
      "Decoder Hidden Shape: torch.Size([2, 5, 500])\n",
      "Target Variable 1:  tensor([   12,  7249, 13126,  2667,    30])\n",
      "Target Variable 1 Shape:  torch.Size([5])\n",
      "Decoder Input Shape:  torch.Size([1, 5])\n",
      "Mask at the present time step:  tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor(50.4219, grad_fn=<AddBackward0>) \n",
      "\n",
      "[50.439839363098145, 50.41316032409668, 50.56408405303955, 50.29135227203369, 50.40099620819092]\n",
      "---------------------------------------- \n",
      "\n",
      "---------------------------------------- \n",
      "\n",
      "Decoder Output Shape: torch.Size([1, 5])\n",
      "Decoder Hidden Shape: torch.Size([2, 5, 500])\n",
      "Target Variable 1:  tensor([45, 17, 29, 12, 72])\n",
      "Target Variable 1 Shape:  torch.Size([5])\n",
      "Decoder Input Shape:  torch.Size([1, 5])\n",
      "Mask at the present time step:  tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor(60.4830, grad_fn=<AddBackward0>) \n",
      "\n",
      "[50.439839363098145, 50.41316032409668, 50.56408405303955, 50.29135227203369, 50.40099620819092, 50.305514335632324]\n",
      "---------------------------------------- \n",
      "\n",
      "---------------------------------------- \n",
      "\n",
      "Decoder Output Shape: torch.Size([1, 5])\n",
      "Decoder Hidden Shape: torch.Size([2, 5, 500])\n",
      "Target Variable 1:  tensor([162, 307,  45,   2,  39])\n",
      "Target Variable 1 Shape:  torch.Size([5])\n",
      "Decoder Input Shape:  torch.Size([1, 5])\n",
      "Mask at the present time step:  tensor([1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor(70.5838, grad_fn=<AddBackward0>) \n",
      "\n",
      "[50.439839363098145, 50.41316032409668, 50.56408405303955, 50.29135227203369, 50.40099620819092, 50.305514335632324, 50.50412654876709]\n",
      "---------------------------------------- \n",
      "\n",
      "---------------------------------------- \n",
      "\n",
      "Decoder Output Shape: torch.Size([1, 5])\n",
      "Decoder Hidden Shape: torch.Size([2, 5, 500])\n",
      "Target Variable 1:  tensor([ 30,  57, 162,   0,  12])\n",
      "Target Variable 1 Shape:  torch.Size([5])\n",
      "Decoder Input Shape:  torch.Size([1, 5])\n",
      "Mask at the present time step:  tensor([1, 1, 1, 0, 1], dtype=torch.uint8)\n",
      "tensor(80.6412, grad_fn=<AddBackward0>) \n",
      "\n",
      "[50.439839363098145, 50.41316032409668, 50.56408405303955, 50.29135227203369, 50.40099620819092, 50.305514335632324, 50.50412654876709, 40.229530334472656]\n",
      "---------------------------------------- \n",
      "\n",
      "---------------------------------------- \n",
      "\n",
      "Decoder Output Shape: torch.Size([1, 5])\n",
      "Decoder Hidden Shape: torch.Size([2, 5, 500])\n",
      "Target Variable 1:  tensor([29, 34, 30,  0,  2])\n",
      "Target Variable 1 Shape:  torch.Size([5])\n",
      "Decoder Input Shape:  torch.Size([1, 5])\n",
      "Mask at the present time step:  tensor([1, 1, 1, 0, 1], dtype=torch.uint8)\n",
      "tensor(90.7123, grad_fn=<AddBackward0>) \n",
      "\n",
      "[50.439839363098145, 50.41316032409668, 50.56408405303955, 50.29135227203369, 50.40099620819092, 50.305514335632324, 50.50412654876709, 40.229530334472656, 40.28460693359375]\n",
      "---------------------------------------- \n",
      "\n",
      "---------------------------------------- \n",
      "\n",
      "Decoder Output Shape: torch.Size([1, 5])\n",
      "Decoder Hidden Shape: torch.Size([2, 5, 500])\n",
      "Target Variable 1:  tensor([ 2, 42, 29,  0,  0])\n",
      "Target Variable 1 Shape:  torch.Size([5])\n",
      "Decoder Input Shape:  torch.Size([1, 5])\n",
      "Mask at the present time step:  tensor([1, 1, 1, 0, 0], dtype=torch.uint8)\n",
      "tensor(100.7925, grad_fn=<AddBackward0>) \n",
      "\n",
      "[50.439839363098145, 50.41316032409668, 50.56408405303955, 50.29135227203369, 50.40099620819092, 50.305514335632324, 50.50412654876709, 40.229530334472656, 40.28460693359375, 30.240406036376953]\n",
      "---------------------------------------- \n",
      "\n",
      "---------------------------------------- \n",
      "\n",
      "Decoder Output Shape: torch.Size([1, 5])\n",
      "Decoder Hidden Shape: torch.Size([2, 5, 500])\n",
      "Target Variable 1:  tensor([ 0, 12,  2,  0,  0])\n",
      "Target Variable 1 Shape:  torch.Size([5])\n",
      "Decoder Input Shape:  torch.Size([1, 5])\n",
      "Mask at the present time step:  tensor([0, 1, 1, 0, 0], dtype=torch.uint8)\n",
      "tensor(110.8540, grad_fn=<AddBackward0>) \n",
      "\n",
      "[50.439839363098145, 50.41316032409668, 50.56408405303955, 50.29135227203369, 50.40099620819092, 50.305514335632324, 50.50412654876709, 40.229530334472656, 40.28460693359375, 30.240406036376953, 20.123046875]\n",
      "---------------------------------------- \n",
      "\n",
      "---------------------------------------- \n",
      "\n",
      "Decoder Output Shape: torch.Size([1, 5])\n",
      "Decoder Hidden Shape: torch.Size([2, 5, 500])\n",
      "Target Variable 1:  tensor([0, 2, 0, 0, 0])\n",
      "Target Variable 1 Shape:  torch.Size([5])\n",
      "Decoder Input Shape:  torch.Size([1, 5])\n",
      "Mask at the present time step:  tensor([0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor(120.9057, grad_fn=<AddBackward0>) \n",
      "\n",
      "[50.439839363098145, 50.41316032409668, 50.56408405303955, 50.29135227203369, 50.40099620819092, 50.305514335632324, 50.50412654876709, 40.229530334472656, 40.28460693359375, 30.240406036376953, 20.123046875, 10.051724433898926]\n",
      "---------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    }
   ],
   "source": [
    "small_batch = 5\n",
    "batches = batch2data(vocab, [random.choice(final_pairs) for _ in range(small_batch)])\n",
    "inp, lengths, out, mask, max_target_len = batches\n",
    "\n",
    "print('Input: ', inp)\n",
    "print('Lengths: ', lengths)\n",
    "print('Output: ', out)\n",
    "print('Mask: ', mask)\n",
    "print('Max Len: ', max_target_len)\n",
    "\n",
    "# Defining the parameters:\n",
    "hidden_size = 500\n",
    "n_enc_layers = 2\n",
    "n_dec_layers = 2\n",
    "dropout = 0.1\n",
    "attn_model = 'dot'\n",
    "embedding = nn.Embedding(vocab.num_words, hidden_size)\n",
    "\n",
    "# Defining Encoder and Decoder\n",
    "encoder = EncoderRNN(hidden_size, embedding, n_enc_layers, dropout)\n",
    "decoder = DecoderRNN(attn_model, embedding, hidden_size, vocab.num_words, n_dec_layers, dropout)\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Zero gradients\n",
    "encoder_opt = optim.Adam(encoder.parameters(), lr=0.0001)\n",
    "decoder_opt = optim.Adam(decoder.parameters(), lr=0.0001)\n",
    "encoder_opt.zero_grad()\n",
    "decoder_opt.zero_grad()\n",
    "\n",
    "# Set device options\n",
    "input_variable = inp.to(device)\n",
    "lengths = lengths.to(device)\n",
    "target_variable = out.to(device)\n",
    "mask = mask.to(device)\n",
    "\n",
    "# Initialize variables\n",
    "loss = 0\n",
    "print_losses = []\n",
    "n_totals = 0\n",
    "\n",
    "# Forward pass through encoder\n",
    "encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "# Create initial decoder input (start with SOS tokens for each sentence)\n",
    "decoder_input = torch.LongTensor([[s_token for _ in range(small_batch)]])\n",
    "decoder_input = decoder_input.to(device)\n",
    "\n",
    "# Set initial decoder hidden state to the encoder's final hidden state\n",
    "decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "print(\"Initial Decoder Hidden Shape: \", decoder_hidden.shape)\n",
    "print('\\n')\n",
    "\n",
    "for t in range(max_target_len):\n",
    "    print('---------------------------------------- \\n')\n",
    "    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "    # Teacher forcing: next input is current target\n",
    "    print(\"Decoder Output Shape:\", decoder_input.shape)\n",
    "    print(\"Decoder Hidden Shape:\", decoder_hidden.shape)\n",
    "    decoder_input = target_variable[t].view(1, -1)\n",
    "    print(\"Target Variable 1: \", target_variable[t])\n",
    "    print(\"Target Variable 1 Shape: \", target_variable[t].shape)\n",
    "    print(\"Decoder Input Shape: \", decoder_input.shape)\n",
    "    # Calculate and accumulate loss\n",
    "    print(\"Mask at the present time step: \", mask[t])\n",
    "    mask_loss, nTotal = maskedNLL(decoder_output, target_variable[t], mask[t])\n",
    "    loss += mask_loss\n",
    "    print(loss,'\\n')\n",
    "    print_losses.append(mask_loss.item() * nTotal)\n",
    "    n_totals += nTotal\n",
    "    print(print_losses)\n",
    "    print('---------------------------------------- \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=13):\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[s_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "    # Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskedNLL(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskedNLL(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # Perform backpropatation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
    "\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch2data(voc, [random.choice(pairs) for _ in range(batch_size)]) for _ in range(n_iteration)]\n",
    "\n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        # Extract fields from batch\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (iteration % save_every == 0):\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * s_token\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # Return collections of word tokens and scores\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=13):\n",
    "    ### Format input sentence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [sent_idxs(voc, sentence)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # Use appropriate device\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # Decode sentence with searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    # indexes -> words\n",
    "    decoded_words = [voc.idx2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input('> ')\n",
    "            # Check if it is quit case\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': \n",
    "                break\n",
    "            # Normalize sentence\n",
    "            input_sentence = normalise(input_sentence)\n",
    "            # Evaluate sentence\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            # Format and print response sentence\n",
    "            output_words[:] = [x for x in output_words if (x != 'eos' and x != 'pad')]\n",
    "            print('Bot: ', ' '.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "# Configure models\n",
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "#attn_model = 'general'\n",
    "#attn_model = 'concat'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "save_dir = 'model_weights'\n",
    "corpus_name = 'cornell_movie'\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "loadFilename = None\n",
    "checkpoint_iter = 4000\n",
    "#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
    "#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "\n",
    "# Load model if a loadFilename is provided\n",
    "if loadFilename:\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint['en']\n",
    "    decoder_sd = checkpoint['de']\n",
    "    encoder_optimizer_sd = checkpoint['en_opt']\n",
    "    decoder_optimizer_sd = checkpoint['de_opt']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    voc.__dict__ = checkpoint['voc_dict']\n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(vocab.num_words, hidden_size)\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = DecoderRNN(attn_model, embedding, hidden_size, vocab.num_words, decoder_n_layers, dropout)\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers ...\n",
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n",
      "Iteration: 1; Percent complete: 0.2%; Average loss: 3.0061\n",
      "Iteration: 2; Percent complete: 0.4%; Average loss: 3.0825\n",
      "Iteration: 3; Percent complete: 0.6%; Average loss: 3.2505\n",
      "Iteration: 4; Percent complete: 0.8%; Average loss: 3.1688\n",
      "Iteration: 5; Percent complete: 1.0%; Average loss: 3.1822\n",
      "Iteration: 6; Percent complete: 1.2%; Average loss: 3.2671\n",
      "Iteration: 7; Percent complete: 1.4%; Average loss: 3.2452\n",
      "Iteration: 8; Percent complete: 1.6%; Average loss: 3.3508\n",
      "Iteration: 9; Percent complete: 1.8%; Average loss: 3.2280\n",
      "Iteration: 10; Percent complete: 2.0%; Average loss: 3.0722\n",
      "Iteration: 11; Percent complete: 2.2%; Average loss: 3.0252\n",
      "Iteration: 12; Percent complete: 2.4%; Average loss: 3.1291\n",
      "Iteration: 13; Percent complete: 2.6%; Average loss: 3.2354\n",
      "Iteration: 14; Percent complete: 2.8%; Average loss: 3.3503\n",
      "Iteration: 15; Percent complete: 3.0%; Average loss: 3.2176\n",
      "Iteration: 16; Percent complete: 3.2%; Average loss: 3.1285\n",
      "Iteration: 17; Percent complete: 3.4%; Average loss: 3.2026\n",
      "Iteration: 18; Percent complete: 3.6%; Average loss: 3.3467\n",
      "Iteration: 19; Percent complete: 3.8%; Average loss: 3.3523\n",
      "Iteration: 20; Percent complete: 4.0%; Average loss: 3.0389\n",
      "Iteration: 21; Percent complete: 4.2%; Average loss: 3.3927\n",
      "Iteration: 22; Percent complete: 4.4%; Average loss: 3.2875\n",
      "Iteration: 23; Percent complete: 4.6%; Average loss: 3.0177\n",
      "Iteration: 24; Percent complete: 4.8%; Average loss: 3.3459\n",
      "Iteration: 25; Percent complete: 5.0%; Average loss: 3.4102\n",
      "Iteration: 26; Percent complete: 5.2%; Average loss: 3.4413\n",
      "Iteration: 27; Percent complete: 5.4%; Average loss: 3.2479\n",
      "Iteration: 28; Percent complete: 5.6%; Average loss: 3.0563\n",
      "Iteration: 29; Percent complete: 5.8%; Average loss: 3.1330\n",
      "Iteration: 30; Percent complete: 6.0%; Average loss: 3.1432\n",
      "Iteration: 31; Percent complete: 6.2%; Average loss: 3.5838\n",
      "Iteration: 32; Percent complete: 6.4%; Average loss: 3.2316\n",
      "Iteration: 33; Percent complete: 6.6%; Average loss: 3.4105\n",
      "Iteration: 34; Percent complete: 6.8%; Average loss: 3.3751\n",
      "Iteration: 35; Percent complete: 7.0%; Average loss: 3.2868\n",
      "Iteration: 36; Percent complete: 7.2%; Average loss: 3.3556\n",
      "Iteration: 37; Percent complete: 7.4%; Average loss: 3.0742\n",
      "Iteration: 38; Percent complete: 7.6%; Average loss: 3.3441\n",
      "Iteration: 39; Percent complete: 7.8%; Average loss: 3.3604\n",
      "Iteration: 40; Percent complete: 8.0%; Average loss: 3.1248\n",
      "Iteration: 41; Percent complete: 8.2%; Average loss: 3.1823\n",
      "Iteration: 42; Percent complete: 8.4%; Average loss: 3.1580\n",
      "Iteration: 43; Percent complete: 8.6%; Average loss: 3.1399\n",
      "Iteration: 44; Percent complete: 8.8%; Average loss: 3.0174\n",
      "Iteration: 45; Percent complete: 9.0%; Average loss: 3.1035\n",
      "Iteration: 46; Percent complete: 9.2%; Average loss: 3.0932\n",
      "Iteration: 47; Percent complete: 9.4%; Average loss: 2.9429\n",
      "Iteration: 48; Percent complete: 9.6%; Average loss: 3.1909\n",
      "Iteration: 49; Percent complete: 9.8%; Average loss: 3.0789\n",
      "Iteration: 50; Percent complete: 10.0%; Average loss: 3.2576\n",
      "Iteration: 51; Percent complete: 10.2%; Average loss: 3.1469\n",
      "Iteration: 52; Percent complete: 10.4%; Average loss: 3.3118\n",
      "Iteration: 53; Percent complete: 10.6%; Average loss: 2.8904\n",
      "Iteration: 54; Percent complete: 10.8%; Average loss: 3.3377\n",
      "Iteration: 55; Percent complete: 11.0%; Average loss: 3.1548\n",
      "Iteration: 56; Percent complete: 11.2%; Average loss: 3.0928\n",
      "Iteration: 57; Percent complete: 11.4%; Average loss: 3.2446\n",
      "Iteration: 58; Percent complete: 11.6%; Average loss: 3.3604\n",
      "Iteration: 59; Percent complete: 11.8%; Average loss: 3.2895\n",
      "Iteration: 60; Percent complete: 12.0%; Average loss: 3.4291\n",
      "Iteration: 61; Percent complete: 12.2%; Average loss: 3.0442\n",
      "Iteration: 62; Percent complete: 12.4%; Average loss: 3.2595\n",
      "Iteration: 63; Percent complete: 12.6%; Average loss: 3.0794\n",
      "Iteration: 64; Percent complete: 12.8%; Average loss: 3.1920\n",
      "Iteration: 65; Percent complete: 13.0%; Average loss: 3.4338\n",
      "Iteration: 66; Percent complete: 13.2%; Average loss: 3.2290\n",
      "Iteration: 67; Percent complete: 13.4%; Average loss: 2.8470\n",
      "Iteration: 68; Percent complete: 13.6%; Average loss: 3.1449\n",
      "Iteration: 69; Percent complete: 13.8%; Average loss: 3.3826\n",
      "Iteration: 70; Percent complete: 14.0%; Average loss: 3.2369\n",
      "Iteration: 71; Percent complete: 14.2%; Average loss: 2.8298\n",
      "Iteration: 72; Percent complete: 14.4%; Average loss: 2.8487\n",
      "Iteration: 73; Percent complete: 14.6%; Average loss: 3.3214\n",
      "Iteration: 74; Percent complete: 14.8%; Average loss: 3.2638\n",
      "Iteration: 75; Percent complete: 15.0%; Average loss: 3.2605\n",
      "Iteration: 76; Percent complete: 15.2%; Average loss: 3.2119\n",
      "Iteration: 77; Percent complete: 15.4%; Average loss: 2.8472\n",
      "Iteration: 78; Percent complete: 15.6%; Average loss: 3.2681\n",
      "Iteration: 79; Percent complete: 15.8%; Average loss: 3.2538\n",
      "Iteration: 80; Percent complete: 16.0%; Average loss: 2.8938\n",
      "Iteration: 81; Percent complete: 16.2%; Average loss: 3.1583\n",
      "Iteration: 82; Percent complete: 16.4%; Average loss: 2.9327\n",
      "Iteration: 83; Percent complete: 16.6%; Average loss: 3.2310\n",
      "Iteration: 84; Percent complete: 16.8%; Average loss: 3.0966\n",
      "Iteration: 85; Percent complete: 17.0%; Average loss: 3.2330\n",
      "Iteration: 86; Percent complete: 17.2%; Average loss: 3.0045\n",
      "Iteration: 87; Percent complete: 17.4%; Average loss: 3.2261\n",
      "Iteration: 88; Percent complete: 17.6%; Average loss: 3.1679\n",
      "Iteration: 89; Percent complete: 17.8%; Average loss: 3.2408\n",
      "Iteration: 90; Percent complete: 18.0%; Average loss: 3.1367\n",
      "Iteration: 91; Percent complete: 18.2%; Average loss: 3.3377\n",
      "Iteration: 92; Percent complete: 18.4%; Average loss: 3.1136\n",
      "Iteration: 93; Percent complete: 18.6%; Average loss: 3.1757\n",
      "Iteration: 94; Percent complete: 18.8%; Average loss: 3.2476\n",
      "Iteration: 95; Percent complete: 19.0%; Average loss: 3.3235\n",
      "Iteration: 96; Percent complete: 19.2%; Average loss: 3.1825\n",
      "Iteration: 97; Percent complete: 19.4%; Average loss: 2.9528\n",
      "Iteration: 98; Percent complete: 19.6%; Average loss: 3.2860\n",
      "Iteration: 99; Percent complete: 19.8%; Average loss: 2.9823\n",
      "Iteration: 100; Percent complete: 20.0%; Average loss: 3.3096\n",
      "Iteration: 101; Percent complete: 20.2%; Average loss: 3.1386\n",
      "Iteration: 102; Percent complete: 20.4%; Average loss: 3.1315\n",
      "Iteration: 103; Percent complete: 20.6%; Average loss: 3.1779\n",
      "Iteration: 104; Percent complete: 20.8%; Average loss: 3.1726\n",
      "Iteration: 105; Percent complete: 21.0%; Average loss: 3.2239\n",
      "Iteration: 106; Percent complete: 21.2%; Average loss: 3.0836\n",
      "Iteration: 107; Percent complete: 21.4%; Average loss: 3.1786\n",
      "Iteration: 108; Percent complete: 21.6%; Average loss: 3.3305\n",
      "Iteration: 109; Percent complete: 21.8%; Average loss: 3.1296\n",
      "Iteration: 110; Percent complete: 22.0%; Average loss: 3.0309\n",
      "Iteration: 111; Percent complete: 22.2%; Average loss: 3.4033\n",
      "Iteration: 112; Percent complete: 22.4%; Average loss: 3.3750\n",
      "Iteration: 113; Percent complete: 22.6%; Average loss: 3.0447\n",
      "Iteration: 114; Percent complete: 22.8%; Average loss: 3.1847\n",
      "Iteration: 115; Percent complete: 23.0%; Average loss: 2.9011\n",
      "Iteration: 116; Percent complete: 23.2%; Average loss: 3.2643\n",
      "Iteration: 117; Percent complete: 23.4%; Average loss: 3.4280\n",
      "Iteration: 118; Percent complete: 23.6%; Average loss: 3.0636\n",
      "Iteration: 119; Percent complete: 23.8%; Average loss: 3.1334\n",
      "Iteration: 120; Percent complete: 24.0%; Average loss: 3.1978\n",
      "Iteration: 121; Percent complete: 24.2%; Average loss: 2.9602\n",
      "Iteration: 122; Percent complete: 24.4%; Average loss: 3.2368\n",
      "Iteration: 123; Percent complete: 24.6%; Average loss: 3.2045\n",
      "Iteration: 124; Percent complete: 24.8%; Average loss: 3.1949\n",
      "Iteration: 125; Percent complete: 25.0%; Average loss: 3.1474\n",
      "Iteration: 126; Percent complete: 25.2%; Average loss: 3.1890\n",
      "Iteration: 127; Percent complete: 25.4%; Average loss: 2.9928\n",
      "Iteration: 128; Percent complete: 25.6%; Average loss: 3.2283\n",
      "Iteration: 129; Percent complete: 25.8%; Average loss: 2.9188\n",
      "Iteration: 130; Percent complete: 26.0%; Average loss: 3.2392\n",
      "Iteration: 131; Percent complete: 26.2%; Average loss: 3.2693\n",
      "Iteration: 132; Percent complete: 26.4%; Average loss: 3.3029\n",
      "Iteration: 133; Percent complete: 26.6%; Average loss: 3.3999\n",
      "Iteration: 134; Percent complete: 26.8%; Average loss: 3.4116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 135; Percent complete: 27.0%; Average loss: 3.0716\n",
      "Iteration: 136; Percent complete: 27.2%; Average loss: 3.1019\n",
      "Iteration: 137; Percent complete: 27.4%; Average loss: 3.1014\n",
      "Iteration: 138; Percent complete: 27.6%; Average loss: 3.2437\n",
      "Iteration: 139; Percent complete: 27.8%; Average loss: 3.0540\n",
      "Iteration: 140; Percent complete: 28.0%; Average loss: 3.0099\n",
      "Iteration: 141; Percent complete: 28.2%; Average loss: 2.9319\n",
      "Iteration: 142; Percent complete: 28.4%; Average loss: 3.0535\n",
      "Iteration: 143; Percent complete: 28.6%; Average loss: 2.9843\n",
      "Iteration: 144; Percent complete: 28.8%; Average loss: 3.3624\n",
      "Iteration: 145; Percent complete: 29.0%; Average loss: 2.9518\n",
      "Iteration: 146; Percent complete: 29.2%; Average loss: 3.4636\n",
      "Iteration: 147; Percent complete: 29.4%; Average loss: 2.9907\n",
      "Iteration: 148; Percent complete: 29.6%; Average loss: 3.4317\n",
      "Iteration: 149; Percent complete: 29.8%; Average loss: 2.8928\n",
      "Iteration: 150; Percent complete: 30.0%; Average loss: 3.2364\n",
      "Iteration: 151; Percent complete: 30.2%; Average loss: 2.9726\n",
      "Iteration: 152; Percent complete: 30.4%; Average loss: 3.0639\n",
      "Iteration: 153; Percent complete: 30.6%; Average loss: 3.1085\n",
      "Iteration: 154; Percent complete: 30.8%; Average loss: 3.1028\n",
      "Iteration: 155; Percent complete: 31.0%; Average loss: 3.0512\n",
      "Iteration: 156; Percent complete: 31.2%; Average loss: 3.1301\n",
      "Iteration: 157; Percent complete: 31.4%; Average loss: 3.1145\n",
      "Iteration: 158; Percent complete: 31.6%; Average loss: 3.2352\n",
      "Iteration: 159; Percent complete: 31.8%; Average loss: 3.2116\n",
      "Iteration: 160; Percent complete: 32.0%; Average loss: 3.0842\n",
      "Iteration: 161; Percent complete: 32.2%; Average loss: 3.2808\n",
      "Iteration: 162; Percent complete: 32.4%; Average loss: 3.1227\n",
      "Iteration: 163; Percent complete: 32.6%; Average loss: 3.4895\n",
      "Iteration: 164; Percent complete: 32.8%; Average loss: 3.3172\n",
      "Iteration: 165; Percent complete: 33.0%; Average loss: 3.3517\n",
      "Iteration: 166; Percent complete: 33.2%; Average loss: 3.1288\n",
      "Iteration: 167; Percent complete: 33.4%; Average loss: 3.0532\n",
      "Iteration: 168; Percent complete: 33.6%; Average loss: 2.8968\n",
      "Iteration: 169; Percent complete: 33.8%; Average loss: 3.2000\n",
      "Iteration: 170; Percent complete: 34.0%; Average loss: 3.1057\n",
      "Iteration: 171; Percent complete: 34.2%; Average loss: 2.9858\n",
      "Iteration: 172; Percent complete: 34.4%; Average loss: 3.0953\n",
      "Iteration: 173; Percent complete: 34.6%; Average loss: 3.0940\n",
      "Iteration: 174; Percent complete: 34.8%; Average loss: 3.0818\n",
      "Iteration: 175; Percent complete: 35.0%; Average loss: 2.8383\n",
      "Iteration: 176; Percent complete: 35.2%; Average loss: 2.9872\n",
      "Iteration: 177; Percent complete: 35.4%; Average loss: 2.9657\n",
      "Iteration: 178; Percent complete: 35.6%; Average loss: 3.1792\n",
      "Iteration: 179; Percent complete: 35.8%; Average loss: 3.2567\n",
      "Iteration: 180; Percent complete: 36.0%; Average loss: 2.8795\n",
      "Iteration: 181; Percent complete: 36.2%; Average loss: 3.1908\n",
      "Iteration: 182; Percent complete: 36.4%; Average loss: 3.2473\n",
      "Iteration: 183; Percent complete: 36.6%; Average loss: 2.8825\n",
      "Iteration: 184; Percent complete: 36.8%; Average loss: 2.9523\n",
      "Iteration: 185; Percent complete: 37.0%; Average loss: 3.1681\n",
      "Iteration: 186; Percent complete: 37.2%; Average loss: 3.2138\n",
      "Iteration: 187; Percent complete: 37.4%; Average loss: 3.1385\n",
      "Iteration: 188; Percent complete: 37.6%; Average loss: 3.1571\n",
      "Iteration: 189; Percent complete: 37.8%; Average loss: 3.2350\n",
      "Iteration: 190; Percent complete: 38.0%; Average loss: 3.0844\n",
      "Iteration: 191; Percent complete: 38.2%; Average loss: 3.1568\n",
      "Iteration: 192; Percent complete: 38.4%; Average loss: 3.1408\n",
      "Iteration: 193; Percent complete: 38.6%; Average loss: 3.2423\n",
      "Iteration: 194; Percent complete: 38.8%; Average loss: 3.1722\n",
      "Iteration: 195; Percent complete: 39.0%; Average loss: 3.1755\n",
      "Iteration: 196; Percent complete: 39.2%; Average loss: 2.9132\n",
      "Iteration: 197; Percent complete: 39.4%; Average loss: 3.2396\n",
      "Iteration: 198; Percent complete: 39.6%; Average loss: 3.1921\n",
      "Iteration: 199; Percent complete: 39.8%; Average loss: 3.0185\n",
      "Iteration: 200; Percent complete: 40.0%; Average loss: 3.2115\n",
      "Iteration: 201; Percent complete: 40.2%; Average loss: 3.3741\n",
      "Iteration: 202; Percent complete: 40.4%; Average loss: 2.9458\n",
      "Iteration: 203; Percent complete: 40.6%; Average loss: 3.2394\n",
      "Iteration: 204; Percent complete: 40.8%; Average loss: 3.2620\n",
      "Iteration: 205; Percent complete: 41.0%; Average loss: 3.3455\n",
      "Iteration: 206; Percent complete: 41.2%; Average loss: 3.2345\n",
      "Iteration: 207; Percent complete: 41.4%; Average loss: 2.9906\n",
      "Iteration: 208; Percent complete: 41.6%; Average loss: 2.8454\n",
      "Iteration: 209; Percent complete: 41.8%; Average loss: 3.0741\n",
      "Iteration: 210; Percent complete: 42.0%; Average loss: 3.0612\n",
      "Iteration: 211; Percent complete: 42.2%; Average loss: 3.3517\n",
      "Iteration: 212; Percent complete: 42.4%; Average loss: 3.0881\n",
      "Iteration: 213; Percent complete: 42.6%; Average loss: 3.1427\n",
      "Iteration: 214; Percent complete: 42.8%; Average loss: 3.1066\n",
      "Iteration: 215; Percent complete: 43.0%; Average loss: 3.1607\n",
      "Iteration: 216; Percent complete: 43.2%; Average loss: 3.1014\n",
      "Iteration: 217; Percent complete: 43.4%; Average loss: 3.1192\n",
      "Iteration: 218; Percent complete: 43.6%; Average loss: 2.8754\n",
      "Iteration: 219; Percent complete: 43.8%; Average loss: 3.2662\n",
      "Iteration: 220; Percent complete: 44.0%; Average loss: 2.8236\n",
      "Iteration: 221; Percent complete: 44.2%; Average loss: 3.2362\n",
      "Iteration: 222; Percent complete: 44.4%; Average loss: 3.4940\n",
      "Iteration: 223; Percent complete: 44.6%; Average loss: 3.4532\n",
      "Iteration: 224; Percent complete: 44.8%; Average loss: 3.1796\n",
      "Iteration: 225; Percent complete: 45.0%; Average loss: 3.1135\n",
      "Iteration: 226; Percent complete: 45.2%; Average loss: 3.0526\n",
      "Iteration: 227; Percent complete: 45.4%; Average loss: 3.1598\n",
      "Iteration: 228; Percent complete: 45.6%; Average loss: 3.1279\n",
      "Iteration: 229; Percent complete: 45.8%; Average loss: 2.8627\n",
      "Iteration: 230; Percent complete: 46.0%; Average loss: 3.1173\n",
      "Iteration: 231; Percent complete: 46.2%; Average loss: 3.3084\n",
      "Iteration: 232; Percent complete: 46.4%; Average loss: 2.9549\n",
      "Iteration: 233; Percent complete: 46.6%; Average loss: 3.2732\n",
      "Iteration: 234; Percent complete: 46.8%; Average loss: 3.2965\n",
      "Iteration: 235; Percent complete: 47.0%; Average loss: 2.9854\n",
      "Iteration: 236; Percent complete: 47.2%; Average loss: 3.1281\n",
      "Iteration: 237; Percent complete: 47.4%; Average loss: 2.8610\n",
      "Iteration: 238; Percent complete: 47.6%; Average loss: 3.1988\n",
      "Iteration: 239; Percent complete: 47.8%; Average loss: 3.1751\n",
      "Iteration: 240; Percent complete: 48.0%; Average loss: 3.0863\n",
      "Iteration: 241; Percent complete: 48.2%; Average loss: 3.0668\n",
      "Iteration: 242; Percent complete: 48.4%; Average loss: 3.1795\n",
      "Iteration: 243; Percent complete: 48.6%; Average loss: 3.0575\n",
      "Iteration: 244; Percent complete: 48.8%; Average loss: 3.0404\n",
      "Iteration: 245; Percent complete: 49.0%; Average loss: 3.1998\n",
      "Iteration: 246; Percent complete: 49.2%; Average loss: 3.0852\n",
      "Iteration: 247; Percent complete: 49.4%; Average loss: 3.0673\n",
      "Iteration: 248; Percent complete: 49.6%; Average loss: 2.8694\n",
      "Iteration: 249; Percent complete: 49.8%; Average loss: 3.0255\n",
      "Iteration: 250; Percent complete: 50.0%; Average loss: 3.2418\n",
      "Iteration: 251; Percent complete: 50.2%; Average loss: 3.1174\n",
      "Iteration: 252; Percent complete: 50.4%; Average loss: 2.9081\n",
      "Iteration: 253; Percent complete: 50.6%; Average loss: 2.9192\n",
      "Iteration: 254; Percent complete: 50.8%; Average loss: 2.9283\n",
      "Iteration: 255; Percent complete: 51.0%; Average loss: 3.1461\n",
      "Iteration: 256; Percent complete: 51.2%; Average loss: 3.1219\n",
      "Iteration: 257; Percent complete: 51.4%; Average loss: 3.4036\n",
      "Iteration: 258; Percent complete: 51.6%; Average loss: 3.2642\n",
      "Iteration: 259; Percent complete: 51.8%; Average loss: 3.0533\n",
      "Iteration: 260; Percent complete: 52.0%; Average loss: 3.1007\n",
      "Iteration: 261; Percent complete: 52.2%; Average loss: 2.9345\n",
      "Iteration: 262; Percent complete: 52.4%; Average loss: 3.0391\n",
      "Iteration: 263; Percent complete: 52.6%; Average loss: 2.8920\n",
      "Iteration: 264; Percent complete: 52.8%; Average loss: 3.2230\n",
      "Iteration: 265; Percent complete: 53.0%; Average loss: 3.0955\n",
      "Iteration: 266; Percent complete: 53.2%; Average loss: 3.0481\n",
      "Iteration: 267; Percent complete: 53.4%; Average loss: 3.0523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 268; Percent complete: 53.6%; Average loss: 3.1332\n",
      "Iteration: 269; Percent complete: 53.8%; Average loss: 3.1626\n",
      "Iteration: 270; Percent complete: 54.0%; Average loss: 2.9570\n",
      "Iteration: 271; Percent complete: 54.2%; Average loss: 2.9705\n",
      "Iteration: 272; Percent complete: 54.4%; Average loss: 2.9556\n",
      "Iteration: 273; Percent complete: 54.6%; Average loss: 3.2195\n",
      "Iteration: 274; Percent complete: 54.8%; Average loss: 2.9680\n",
      "Iteration: 275; Percent complete: 55.0%; Average loss: 3.1331\n",
      "Iteration: 276; Percent complete: 55.2%; Average loss: 2.9338\n",
      "Iteration: 277; Percent complete: 55.4%; Average loss: 3.0770\n",
      "Iteration: 278; Percent complete: 55.6%; Average loss: 3.0924\n",
      "Iteration: 279; Percent complete: 55.8%; Average loss: 3.2919\n",
      "Iteration: 280; Percent complete: 56.0%; Average loss: 2.8221\n",
      "Iteration: 281; Percent complete: 56.2%; Average loss: 3.0760\n",
      "Iteration: 282; Percent complete: 56.4%; Average loss: 3.3516\n",
      "Iteration: 283; Percent complete: 56.6%; Average loss: 2.8948\n",
      "Iteration: 284; Percent complete: 56.8%; Average loss: 3.2800\n",
      "Iteration: 285; Percent complete: 57.0%; Average loss: 3.2016\n",
      "Iteration: 286; Percent complete: 57.2%; Average loss: 2.9341\n",
      "Iteration: 287; Percent complete: 57.4%; Average loss: 3.1435\n",
      "Iteration: 288; Percent complete: 57.6%; Average loss: 3.2751\n",
      "Iteration: 289; Percent complete: 57.8%; Average loss: 2.9921\n",
      "Iteration: 290; Percent complete: 58.0%; Average loss: 3.1412\n",
      "Iteration: 291; Percent complete: 58.2%; Average loss: 3.0400\n",
      "Iteration: 292; Percent complete: 58.4%; Average loss: 3.0186\n",
      "Iteration: 293; Percent complete: 58.6%; Average loss: 2.9318\n",
      "Iteration: 294; Percent complete: 58.8%; Average loss: 3.1512\n",
      "Iteration: 295; Percent complete: 59.0%; Average loss: 2.9240\n",
      "Iteration: 296; Percent complete: 59.2%; Average loss: 3.0619\n",
      "Iteration: 297; Percent complete: 59.4%; Average loss: 3.0688\n",
      "Iteration: 298; Percent complete: 59.6%; Average loss: 2.9292\n",
      "Iteration: 299; Percent complete: 59.8%; Average loss: 2.8741\n",
      "Iteration: 300; Percent complete: 60.0%; Average loss: 3.1615\n",
      "Iteration: 301; Percent complete: 60.2%; Average loss: 2.7644\n",
      "Iteration: 302; Percent complete: 60.4%; Average loss: 2.8998\n",
      "Iteration: 303; Percent complete: 60.6%; Average loss: 3.0367\n",
      "Iteration: 304; Percent complete: 60.8%; Average loss: 3.2299\n",
      "Iteration: 305; Percent complete: 61.0%; Average loss: 2.9416\n",
      "Iteration: 306; Percent complete: 61.2%; Average loss: 3.2514\n",
      "Iteration: 307; Percent complete: 61.4%; Average loss: 3.2066\n",
      "Iteration: 308; Percent complete: 61.6%; Average loss: 3.1117\n",
      "Iteration: 309; Percent complete: 61.8%; Average loss: 2.9598\n",
      "Iteration: 310; Percent complete: 62.0%; Average loss: 3.1573\n",
      "Iteration: 311; Percent complete: 62.2%; Average loss: 3.1178\n",
      "Iteration: 312; Percent complete: 62.4%; Average loss: 2.9616\n",
      "Iteration: 313; Percent complete: 62.6%; Average loss: 3.5088\n",
      "Iteration: 314; Percent complete: 62.8%; Average loss: 2.9927\n",
      "Iteration: 315; Percent complete: 63.0%; Average loss: 3.0656\n",
      "Iteration: 316; Percent complete: 63.2%; Average loss: 3.1696\n",
      "Iteration: 317; Percent complete: 63.4%; Average loss: 3.1320\n",
      "Iteration: 318; Percent complete: 63.6%; Average loss: 3.0025\n",
      "Iteration: 319; Percent complete: 63.8%; Average loss: 2.9103\n",
      "Iteration: 320; Percent complete: 64.0%; Average loss: 2.8412\n",
      "Iteration: 321; Percent complete: 64.2%; Average loss: 3.2435\n",
      "Iteration: 322; Percent complete: 64.4%; Average loss: 3.1831\n",
      "Iteration: 323; Percent complete: 64.6%; Average loss: 2.9158\n",
      "Iteration: 324; Percent complete: 64.8%; Average loss: 3.0965\n",
      "Iteration: 325; Percent complete: 65.0%; Average loss: 3.0837\n",
      "Iteration: 326; Percent complete: 65.2%; Average loss: 3.1214\n",
      "Iteration: 327; Percent complete: 65.4%; Average loss: 3.1143\n",
      "Iteration: 328; Percent complete: 65.6%; Average loss: 2.8344\n",
      "Iteration: 329; Percent complete: 65.8%; Average loss: 3.2427\n",
      "Iteration: 330; Percent complete: 66.0%; Average loss: 3.2714\n",
      "Iteration: 331; Percent complete: 66.2%; Average loss: 3.0574\n",
      "Iteration: 332; Percent complete: 66.4%; Average loss: 3.1247\n",
      "Iteration: 333; Percent complete: 66.6%; Average loss: 2.8426\n",
      "Iteration: 334; Percent complete: 66.8%; Average loss: 3.2445\n",
      "Iteration: 335; Percent complete: 67.0%; Average loss: 3.0608\n",
      "Iteration: 336; Percent complete: 67.2%; Average loss: 3.0447\n",
      "Iteration: 337; Percent complete: 67.4%; Average loss: 2.7477\n",
      "Iteration: 338; Percent complete: 67.6%; Average loss: 3.1017\n",
      "Iteration: 339; Percent complete: 67.8%; Average loss: 3.0959\n",
      "Iteration: 340; Percent complete: 68.0%; Average loss: 3.0369\n",
      "Iteration: 341; Percent complete: 68.2%; Average loss: 3.0511\n",
      "Iteration: 342; Percent complete: 68.4%; Average loss: 3.0009\n",
      "Iteration: 343; Percent complete: 68.6%; Average loss: 2.9188\n",
      "Iteration: 344; Percent complete: 68.8%; Average loss: 3.1307\n",
      "Iteration: 345; Percent complete: 69.0%; Average loss: 2.9418\n",
      "Iteration: 346; Percent complete: 69.2%; Average loss: 3.1311\n",
      "Iteration: 347; Percent complete: 69.4%; Average loss: 3.0093\n",
      "Iteration: 348; Percent complete: 69.6%; Average loss: 3.2302\n",
      "Iteration: 349; Percent complete: 69.8%; Average loss: 3.2790\n",
      "Iteration: 350; Percent complete: 70.0%; Average loss: 3.0738\n",
      "Iteration: 351; Percent complete: 70.2%; Average loss: 3.4338\n",
      "Iteration: 352; Percent complete: 70.4%; Average loss: 3.0612\n",
      "Iteration: 353; Percent complete: 70.6%; Average loss: 3.1491\n",
      "Iteration: 354; Percent complete: 70.8%; Average loss: 2.9690\n",
      "Iteration: 355; Percent complete: 71.0%; Average loss: 2.9197\n",
      "Iteration: 356; Percent complete: 71.2%; Average loss: 3.2585\n",
      "Iteration: 357; Percent complete: 71.4%; Average loss: 2.7152\n",
      "Iteration: 358; Percent complete: 71.6%; Average loss: 3.1450\n",
      "Iteration: 359; Percent complete: 71.8%; Average loss: 3.0321\n",
      "Iteration: 360; Percent complete: 72.0%; Average loss: 3.3130\n",
      "Iteration: 361; Percent complete: 72.2%; Average loss: 2.7471\n",
      "Iteration: 362; Percent complete: 72.4%; Average loss: 2.9028\n",
      "Iteration: 363; Percent complete: 72.6%; Average loss: 3.2505\n",
      "Iteration: 364; Percent complete: 72.8%; Average loss: 2.9420\n",
      "Iteration: 365; Percent complete: 73.0%; Average loss: 2.9662\n",
      "Iteration: 366; Percent complete: 73.2%; Average loss: 3.2578\n",
      "Iteration: 367; Percent complete: 73.4%; Average loss: 3.1038\n",
      "Iteration: 368; Percent complete: 73.6%; Average loss: 3.0373\n",
      "Iteration: 369; Percent complete: 73.8%; Average loss: 3.1981\n",
      "Iteration: 370; Percent complete: 74.0%; Average loss: 2.9052\n",
      "Iteration: 371; Percent complete: 74.2%; Average loss: 3.0918\n",
      "Iteration: 372; Percent complete: 74.4%; Average loss: 3.2670\n",
      "Iteration: 373; Percent complete: 74.6%; Average loss: 2.9491\n",
      "Iteration: 374; Percent complete: 74.8%; Average loss: 3.0870\n",
      "Iteration: 375; Percent complete: 75.0%; Average loss: 2.8901\n",
      "Iteration: 376; Percent complete: 75.2%; Average loss: 2.8964\n",
      "Iteration: 377; Percent complete: 75.4%; Average loss: 2.8164\n",
      "Iteration: 378; Percent complete: 75.6%; Average loss: 2.7874\n",
      "Iteration: 379; Percent complete: 75.8%; Average loss: 2.9926\n",
      "Iteration: 380; Percent complete: 76.0%; Average loss: 3.0081\n",
      "Iteration: 381; Percent complete: 76.2%; Average loss: 3.3359\n",
      "Iteration: 382; Percent complete: 76.4%; Average loss: 2.8742\n",
      "Iteration: 383; Percent complete: 76.6%; Average loss: 3.0458\n",
      "Iteration: 384; Percent complete: 76.8%; Average loss: 2.9223\n",
      "Iteration: 385; Percent complete: 77.0%; Average loss: 2.9526\n",
      "Iteration: 386; Percent complete: 77.2%; Average loss: 2.9313\n",
      "Iteration: 387; Percent complete: 77.4%; Average loss: 3.2277\n",
      "Iteration: 388; Percent complete: 77.6%; Average loss: 3.0853\n",
      "Iteration: 389; Percent complete: 77.8%; Average loss: 2.9638\n",
      "Iteration: 390; Percent complete: 78.0%; Average loss: 2.9653\n",
      "Iteration: 391; Percent complete: 78.2%; Average loss: 3.1193\n",
      "Iteration: 392; Percent complete: 78.4%; Average loss: 3.3726\n",
      "Iteration: 393; Percent complete: 78.6%; Average loss: 3.1049\n",
      "Iteration: 394; Percent complete: 78.8%; Average loss: 2.9140\n",
      "Iteration: 395; Percent complete: 79.0%; Average loss: 2.8849\n",
      "Iteration: 396; Percent complete: 79.2%; Average loss: 2.9009\n",
      "Iteration: 397; Percent complete: 79.4%; Average loss: 2.8204\n",
      "Iteration: 398; Percent complete: 79.6%; Average loss: 3.0062\n",
      "Iteration: 399; Percent complete: 79.8%; Average loss: 3.0439\n",
      "Iteration: 400; Percent complete: 80.0%; Average loss: 2.8767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 401; Percent complete: 80.2%; Average loss: 3.0397\n",
      "Iteration: 402; Percent complete: 80.4%; Average loss: 2.8966\n",
      "Iteration: 403; Percent complete: 80.6%; Average loss: 3.1734\n",
      "Iteration: 404; Percent complete: 80.8%; Average loss: 2.9103\n",
      "Iteration: 405; Percent complete: 81.0%; Average loss: 3.0778\n",
      "Iteration: 406; Percent complete: 81.2%; Average loss: 3.0499\n",
      "Iteration: 407; Percent complete: 81.4%; Average loss: 2.7739\n",
      "Iteration: 408; Percent complete: 81.6%; Average loss: 2.7251\n",
      "Iteration: 409; Percent complete: 81.8%; Average loss: 2.7543\n",
      "Iteration: 410; Percent complete: 82.0%; Average loss: 2.8402\n",
      "Iteration: 411; Percent complete: 82.2%; Average loss: 3.2567\n",
      "Iteration: 412; Percent complete: 82.4%; Average loss: 2.8393\n",
      "Iteration: 413; Percent complete: 82.6%; Average loss: 3.0754\n",
      "Iteration: 414; Percent complete: 82.8%; Average loss: 3.2501\n",
      "Iteration: 415; Percent complete: 83.0%; Average loss: 3.0644\n",
      "Iteration: 416; Percent complete: 83.2%; Average loss: 2.7800\n",
      "Iteration: 417; Percent complete: 83.4%; Average loss: 3.0900\n",
      "Iteration: 418; Percent complete: 83.6%; Average loss: 3.0018\n",
      "Iteration: 419; Percent complete: 83.8%; Average loss: 3.0912\n",
      "Iteration: 420; Percent complete: 84.0%; Average loss: 3.0189\n",
      "Iteration: 421; Percent complete: 84.2%; Average loss: 3.1637\n",
      "Iteration: 422; Percent complete: 84.4%; Average loss: 3.2556\n",
      "Iteration: 423; Percent complete: 84.6%; Average loss: 3.4383\n",
      "Iteration: 424; Percent complete: 84.8%; Average loss: 3.0090\n",
      "Iteration: 425; Percent complete: 85.0%; Average loss: 3.1814\n",
      "Iteration: 426; Percent complete: 85.2%; Average loss: 3.0014\n",
      "Iteration: 427; Percent complete: 85.4%; Average loss: 2.7981\n",
      "Iteration: 428; Percent complete: 85.6%; Average loss: 3.2654\n",
      "Iteration: 429; Percent complete: 85.8%; Average loss: 2.7896\n",
      "Iteration: 430; Percent complete: 86.0%; Average loss: 2.9454\n",
      "Iteration: 431; Percent complete: 86.2%; Average loss: 3.0149\n",
      "Iteration: 432; Percent complete: 86.4%; Average loss: 3.1248\n",
      "Iteration: 433; Percent complete: 86.6%; Average loss: 3.1089\n",
      "Iteration: 434; Percent complete: 86.8%; Average loss: 3.1201\n",
      "Iteration: 435; Percent complete: 87.0%; Average loss: 3.1385\n",
      "Iteration: 436; Percent complete: 87.2%; Average loss: 2.9140\n",
      "Iteration: 437; Percent complete: 87.4%; Average loss: 2.9422\n",
      "Iteration: 438; Percent complete: 87.6%; Average loss: 3.0148\n",
      "Iteration: 439; Percent complete: 87.8%; Average loss: 3.4105\n",
      "Iteration: 440; Percent complete: 88.0%; Average loss: 3.0177\n",
      "Iteration: 441; Percent complete: 88.2%; Average loss: 2.8596\n",
      "Iteration: 442; Percent complete: 88.4%; Average loss: 2.9560\n",
      "Iteration: 443; Percent complete: 88.6%; Average loss: 2.8826\n",
      "Iteration: 444; Percent complete: 88.8%; Average loss: 2.8642\n",
      "Iteration: 445; Percent complete: 89.0%; Average loss: 2.9872\n",
      "Iteration: 446; Percent complete: 89.2%; Average loss: 3.1204\n",
      "Iteration: 447; Percent complete: 89.4%; Average loss: 3.0070\n",
      "Iteration: 448; Percent complete: 89.6%; Average loss: 2.9257\n",
      "Iteration: 449; Percent complete: 89.8%; Average loss: 3.0641\n",
      "Iteration: 450; Percent complete: 90.0%; Average loss: 2.9362\n",
      "Iteration: 451; Percent complete: 90.2%; Average loss: 2.9744\n",
      "Iteration: 452; Percent complete: 90.4%; Average loss: 3.0221\n",
      "Iteration: 453; Percent complete: 90.6%; Average loss: 2.9030\n",
      "Iteration: 454; Percent complete: 90.8%; Average loss: 2.9816\n",
      "Iteration: 455; Percent complete: 91.0%; Average loss: 3.1772\n",
      "Iteration: 456; Percent complete: 91.2%; Average loss: 2.8258\n",
      "Iteration: 457; Percent complete: 91.4%; Average loss: 2.9223\n",
      "Iteration: 458; Percent complete: 91.6%; Average loss: 2.7580\n",
      "Iteration: 459; Percent complete: 91.8%; Average loss: 2.7274\n",
      "Iteration: 460; Percent complete: 92.0%; Average loss: 2.9828\n",
      "Iteration: 461; Percent complete: 92.2%; Average loss: 2.8987\n",
      "Iteration: 462; Percent complete: 92.4%; Average loss: 2.7819\n",
      "Iteration: 463; Percent complete: 92.6%; Average loss: 3.0997\n",
      "Iteration: 464; Percent complete: 92.8%; Average loss: 3.2775\n",
      "Iteration: 465; Percent complete: 93.0%; Average loss: 2.8794\n",
      "Iteration: 466; Percent complete: 93.2%; Average loss: 2.9807\n",
      "Iteration: 467; Percent complete: 93.4%; Average loss: 3.0075\n",
      "Iteration: 468; Percent complete: 93.6%; Average loss: 2.9684\n",
      "Iteration: 469; Percent complete: 93.8%; Average loss: 2.7009\n",
      "Iteration: 470; Percent complete: 94.0%; Average loss: 3.0966\n",
      "Iteration: 471; Percent complete: 94.2%; Average loss: 3.1222\n",
      "Iteration: 472; Percent complete: 94.4%; Average loss: 2.8124\n",
      "Iteration: 473; Percent complete: 94.6%; Average loss: 2.8639\n",
      "Iteration: 474; Percent complete: 94.8%; Average loss: 2.7753\n",
      "Iteration: 475; Percent complete: 95.0%; Average loss: 3.0878\n",
      "Iteration: 476; Percent complete: 95.2%; Average loss: 2.9610\n",
      "Iteration: 477; Percent complete: 95.4%; Average loss: 2.8455\n",
      "Iteration: 478; Percent complete: 95.6%; Average loss: 2.9303\n",
      "Iteration: 479; Percent complete: 95.8%; Average loss: 3.0820\n",
      "Iteration: 480; Percent complete: 96.0%; Average loss: 3.0859\n",
      "Iteration: 481; Percent complete: 96.2%; Average loss: 3.2490\n",
      "Iteration: 482; Percent complete: 96.4%; Average loss: 3.0577\n",
      "Iteration: 483; Percent complete: 96.6%; Average loss: 3.0134\n",
      "Iteration: 484; Percent complete: 96.8%; Average loss: 3.0400\n",
      "Iteration: 485; Percent complete: 97.0%; Average loss: 3.0668\n",
      "Iteration: 486; Percent complete: 97.2%; Average loss: 2.9139\n",
      "Iteration: 487; Percent complete: 97.4%; Average loss: 2.9735\n",
      "Iteration: 488; Percent complete: 97.6%; Average loss: 3.0540\n",
      "Iteration: 489; Percent complete: 97.8%; Average loss: 3.1657\n",
      "Iteration: 490; Percent complete: 98.0%; Average loss: 2.9044\n",
      "Iteration: 491; Percent complete: 98.2%; Average loss: 3.0424\n",
      "Iteration: 492; Percent complete: 98.4%; Average loss: 2.8202\n",
      "Iteration: 493; Percent complete: 98.6%; Average loss: 3.1537\n",
      "Iteration: 494; Percent complete: 98.8%; Average loss: 3.2287\n",
      "Iteration: 495; Percent complete: 99.0%; Average loss: 2.6810\n",
      "Iteration: 496; Percent complete: 99.2%; Average loss: 2.7933\n",
      "Iteration: 497; Percent complete: 99.4%; Average loss: 3.0854\n",
      "Iteration: 498; Percent complete: 99.6%; Average loss: 2.9294\n",
      "Iteration: 499; Percent complete: 99.8%; Average loss: 3.2110\n",
      "Iteration: 500; Percent complete: 100.0%; Average loss: 3.0836\n"
     ]
    }
   ],
   "source": [
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 500\n",
    "print_every = 1\n",
    "save_every = 500\n",
    "\n",
    "# Ensure dropout layers are in train mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# If you have cuda, configure cuda to call\n",
    "for state in encoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "for state in decoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "# Run training iterations\n",
    "print(\"Starting Training!\")\n",
    "trainIters(model_name, vocab, final_pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder, 'model_weights/enc_weights500')\n",
    "torch.save(decoder, 'model_weights/dec_weights500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> hi\n",
      "Bot:  hi .\n",
      "> how are you?\n",
      "Bot:  i m not going to go .\n",
      "> go where?\n",
      "Bot:  i m not going to go .\n",
      "> what?\n",
      "Bot:  you re not going to go .\n",
      "> i am not\n",
      "Bot:  you re not going to go .\n",
      "> okay\n",
      "Bot:  i m not going to go .\n",
      "> okay\n",
      "Bot:  i m not going to go .\n",
      "> do not go\n",
      "Bot:  i m not going to go .\n",
      "> that is good\n",
      "Bot:  i m not going to go .\n",
      "> say something good\n",
      "Bot:  i m not going to go .\n",
      "> else\n",
      "Bot:  i m not going to go .\n",
      "> bye\n",
      "Bot:  i m not going to go .\n",
      "> byeee\n",
      "Error: Encountered unknown word.\n",
      "> byeeeee\n",
      "Error: Encountered unknown word.\n",
      "> goodbye\n",
      "Bot:  i m not going to go .\n",
      "> quit\n"
     ]
    }
   ],
   "source": [
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "evaluateInput(encoder, decoder, searcher, vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
